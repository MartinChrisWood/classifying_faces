{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Classifying facial expressions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "# Various components of an ML pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# For feature engineering\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# And finally, a few actual models\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper, reading files and performing some pre-processing\n",
    "def load_features_and_labels(subject, expression, path = \"./data/\"):\n",
    "    \"\"\" Reads the text files of raw data for a specified subject and\n",
    "    expression.  Also removes the timestamp column and converts to\n",
    "    numpy arrays for sklearn compatibility.\n",
    "    \"\"\"\n",
    "    raw_features = pd.read_table(path + subject + \"_\" + expression + \"_datapoints.txt\",\n",
    "                                 sep=\" \",\n",
    "                                 header=0)\n",
    "    \n",
    "    # Drop the timestamp!\n",
    "    raw_features = raw_features.drop('0.0', axis=1)\n",
    "    \n",
    "    raw_labels = pd.read_table(path + subject + \"_\" + expression + \"_targets.txt\",\n",
    "                               sep=\" \",\n",
    "                               header=None)\n",
    "    \n",
    "    # Trainsform to numpy arrays\n",
    "    return np.asarray(raw_features), np.ravel(raw_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a & b)  Classify two expressions from subject A using \"off the shelf\" tech\n",
    "\n",
    "NB;  Extra marks for coding my own implementation of the classifier\n",
    "\n",
    "Initial SVM (rbf) parameters taken from https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
    "\n",
    "Use these classifiers to classify two expressions from subject B.\n",
    "\n",
    "NB;  Extra marks for using something beyond simple accuracy (recycle ROC curve code?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying \"doubt\" expression\n",
    "\n",
    "This expression was chosen because in the original paper their own classification pipeline performed really well on it (f1 of 0.88 , precision of 0.94 and recall of 0.76 classifying single frames independently, engineered features WITHOUT depth features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "X_A_doubt, y_A_doubt = load_features_and_labels(\"a\", \"doubt_question\")\n",
    "X_B_doubt, y_B_doubt = load_features_and_labels(\"b\", \"doubt_question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose SVC pipeline and parameters to search\n",
    "svc_pipeline = Pipeline([('SVM', SVC(kernel=\"linear\"))])\n",
    "svc_parameters = {'SVM__C':np.logspace(-2, 10, 13)}\n",
    "\n",
    "# Set up a cross-validated (10-fold) grid search\n",
    "svc_grid = GridSearchCV(svc_pipeline,\n",
    "                        param_grid = svc_parameters,\n",
    "                        cv=3,\n",
    "                        scoring='f1',\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 13 candidates, totalling 39 fits\n"
     ]
    }
   ],
   "source": [
    "# Fit the model!\n",
    "svc_grid.fit(X_A_doubt, y_A_doubt)\n",
    "\n",
    "# Report on performance!\n",
    "scv_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report in some actually useful detail\n",
    "y_train_pred = svc_grid.predict(X_A_doubt)\n",
    "confusion_matrix(y_A_doubt, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the \"doubt\" expression for subject B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report performance against unseen test data\n",
    "y_test_pred = svc_grid.predict(X_B_doubt)\n",
    "confusion_matrix(y_B_doubt, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying \"negative\" expression\n",
    "\n",
    "The original author's model performed poorly on this one (f1 score of 0.44, precision of 0.33, recall of 0.66), so after the apparently easier problem of classifying doubt this should give us our relative performance on a hard problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "X_A_neg, y_A_neg = load_features_and_labels(\"a\", \"negative\")\n",
    "X_B_neg, y_B_neg = load_features_and_labels(\"b\", \"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose SVC pipeline and parameters to search\n",
    "svc_pipeline = Pipeline([('SVM', SVC(kernel=\"linear\"))])\n",
    "svc_parameters = {'SVM__C':np.logspace(-2, 10, 13)}\n",
    "\n",
    "# Set up a cross-validated (10-fold) grid search\n",
    "svc_grid = GridSearchCV(svc_pipeline,\n",
    "                        param_grid = svc_parameters,\n",
    "                        cv=10,\n",
    "                        scoring='f1',\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model!\n",
    "svc_grid.fit(X_A_neg, y_A_neg)\n",
    "\n",
    "# Report on performance!\n",
    "svc_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report in some actually useful detail\n",
    "y_train_pred = svc_grid.predict(X_A_neg)\n",
    "confusion_matrix(y_A_neg, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the \"negative\" expression for subject B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report performance against unseen test data\n",
    "y_test_pred = svc_grid.predict(X_B_neg)\n",
    "confusion_matrix(y_B_neg, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c)  Additional Analysis of classifiers - reverse roles!\n",
    "Train on B, classify A, comment on difference!\n",
    "Try again using a different feature representation (eg; PCA.  Can I think of something better?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)  Implement a different classifier\n",
    "Training on single expression, testing (on B?) and extra marks for own implementation\n",
    "Training and testing on a SECOND expression\n",
    "The same by inverting roles\n",
    "Repeating with a different feature representation again!  \n",
    "REF;  for last, try multiclass classifier on principle there should be some shared information?  Data imbalance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e)  Wrap-up, compare results of the two classifiers, make comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
